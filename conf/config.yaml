random_seed: 42
training:
  normalization_stats_path: normalization_stats.npz
  tensorboard_log_dir: logs/%Y-%m-%d_%H-%M-%S/
  dataset_path: calvin/dataset/calvin_debug_dataset
  relative_actions: True
  gcbc_window_length_min: 32
  gcbc_window_length_max: 64
  lmp_window_length: 64
  batch_size: 256
  mixed_precision_policy: p=f32,c=bf16
  optimizer:
    _target_: optax.chain
    _args_:
      - _target_: optax.clip_by_global_norm
        max_norm: 1.0
      - _target_: optax.adamw
        learning_rate: 1e-3
  num_steps: 50000
  method: play-lmp
  beta: 0.01
  evaluation:
    every_n_steps: 1000
    model_save_path: model.eqx
    num_val_batches: 256
model:
  d_obs: 39
  d_latent: 64
  d_action: 7
  target_action_max:
    _target_: jax.numpy.ones
    _args_:
      - ${model.d_action}
  target_action_min:
    _target_: jax.numpy.full
    _args_:
      - ${model.d_action}
      - -1.0
  plan_recognizer:
    _partial_: true
    _target_: play_lmp.encoders.BidirectionalLSTMPlanRecognitionNetwork
    d_obs: ${model.d_obs}
    d_action: ${model.d_action}
    d_latent: ${model.d_latent}
    hidden_size: 512
  plan_proposal:
    _partial_: true
    _target_: play_lmp.encoders.MLPPlanProposalNetwork
    d_obs: ${model.d_obs}
    d_latent: ${model.d_latent}
    width_size: 512
    depth: 3
  policy:
    _partial_: true
    _target_: play_lmp.policies.LSTMPolicyNetwork
    d_obs: ${model.d_obs}
    d_latent_plan: ${model.d_latent}
    d_action: ${model.d_action}
    hidden_size: 1024
    num_dl_mixture_elements: 6
    action_max_bound: ${model.target_action_max}
    action_min_bound: ${model.target_action_min}
    num_action_bins: 256
