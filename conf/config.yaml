random_seed: 42
training:
  tf_dataset_path: data/language_table_blocktoblock_4block_sim/
  shuffle_buffer_size: 1024
  val_split_proportion: 0.02
  test_split_proportion: 0.05
  normalization_stats_path: normalization_stats.npz
  tensorboard_log_dir: logs/%Y-%m-%d_%H-%M-%S/
  sequence_padding_length: 32
  batch_size: 32
  evaluate_every_n_steps: 100
  optimizer:
    _target_: optax.adamw
    learning_rate: 5e-4
  num_steps: 1000
  method: play-gcbc
  beta: 0.01
model:
  d_proprio: 2
  d_latent: 256
  target_action_max:
    _target_: jax.numpy.asarray
    _args_:
      - [1.0, 1.0]
  target_action_min:
    _target_: jax.numpy.asarray
    _args_:
      - [-1.0, -1.0]
  cnn:
    _partial_: true
    _target_: play_lmp.CNNEncoder
    final_dim: 64
  plan_recognizer:
    _partial_: true
    _target_: play_lmp.PlanRecognitionTransformer
    num_layers: 6
    d_proprio: ${model.d_proprio}
    num_heads: 8
    ff_dim: 256
    d_latent: ${model.d_latent}
    rope_theta: 10000.0
  plan_proposal:
    _partial_: true
    _target_: play_lmp.MLPPlanProposalNetwork
    d_proprio: ${model.d_proprio}
    d_latent: ${model.d_latent}
    width_size: 512
    depth: 3
  policy:
    _partial_: true
    _target_: play_lmp.LSTMPolicyNetwork
    d_proprio: ${model.d_proprio}
    d_latent_plan: ${model.d_latent}
    d_action: 2
    num_dl_mixture_elements: 10
    action_max_bound: ${model.target_action_max}
    action_min_bound: ${model.target_action_min}
    num_action_bins: 64
