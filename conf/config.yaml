random_seed: 42
training:
  normalization_stats_path: normalization_stats.npz
  tensorboard_log_dir: logs/%Y-%m-%d_%H-%M-%S/
  gcbc_window_length_min: 32
  gcbc_window_length_max: 64
  lmp_window_length: 64
  batch_size: 256
  mixed_precision_policy: p=f32,c=bf16
  optimizer:
    _target_: optax.adamw
    learning_rate: 1e-3
  num_steps: 100000
  method: play-gcbc
  beta: 0.01
  evaluate:
    every_n_steps: 1000
    num_episodes: 20
    replan_every_n_steps: 48
model:
  d_obs: 59
  d_goal: 12
  d_latent: 64
  d_action: 9
  target_action_max:
    _target_: jax.numpy.ones
    _args_:
      - ${model.d_action}
  target_action_min:
    _target_: jax.numpy.full
    _args_:
      - ${model.d_action}
      - -1.0
  plan_recognizer:
    _partial_: true
    _target_: play_lmp.PlanRecognitionTransformer
    num_layers: 6
    d_obs: ${model.d_obs}
    num_heads: 8
    ff_dim: 256
    d_latent: ${model.d_latent}
    rope_theta: 10000.0
  plan_proposal:
    _partial_: true
    _target_: play_lmp.MLPPlanProposalNetwork
    d_obs: ${model.d_obs}
    d_goal: ${model.d_goal}
    d_latent: ${model.d_latent}
    width_size: 512
    depth: 3
  policy:
    _partial_: true
    _target_: play_lmp.LSTMPolicyNetwork
    d_obs: ${model.d_obs}
    d_goal: ${model.d_goal}
    d_latent_plan: ${model.d_latent}
    d_action: ${model.d_action}
    hidden_size: 2048
    num_dl_mixture_elements: 6
    action_max_bound: ${model.target_action_max}
    action_min_bound: ${model.target_action_min}
    num_action_bins: 256
